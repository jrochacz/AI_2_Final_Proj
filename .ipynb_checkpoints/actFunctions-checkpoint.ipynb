{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e84050-14fc-4cd3-a7b0-4c5b31c3a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import exp, where, erf, tensor, log, argwhere, ones_like, ones, heaviside, sign\n",
    "from torch import pow as POW\n",
    "from numpy import pi, e\n",
    "from scipy.special import expi\n",
    "from numpy import log as nplog \n",
    "from torch import max as Max\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import erf as sp_erf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdf1d82-ea74-4d08-85a7-c14e958f8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant block cell (Purpose is to work with torch functions and plot the graphs)\n",
    "c = 0\n",
    "if c == 1: \n",
    "    vals = torch.arange(-2., 1., 0.01, dtype=torch.float64)\n",
    "    nvals = -vals\n",
    "    base = 2\n",
    "    svals = torch.randn(len(vals))\n",
    "    print(vals.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194cb31e-94e6-442c-9aa5-d1dd9ee2575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant block cell\n",
    "if c == 1: \n",
    "    plt.grid()\n",
    "    plt.plot(vals, supeReluTest(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e986a8-0e12-4122-af2b-36292ba70f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85c249-01ff-467c-9e1e-3ab3ec803cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c370c615-4533-49f9-a1f3-5f0633ec1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant block cell\n",
    "if c==1:\n",
    "    datax = torch.arange(-10, 5, 0.01, dtype=torch.float64, requires_grad=True) \n",
    "    #datax = tensor([0])\n",
    "    z = 1 \n",
    "    \n",
    "    grad = where(datax < 0, 1-exp(-POW(datax/z, -2)), 1)\n",
    "\n",
    "    grad2 = datax*grad - where(datax < 0, pi**(0.5)*(erf(POW(datax/z, -1))+1), 0)\n",
    "\n",
    "    ddata = torch.where(datax < 0, 1-exp(-(z/datax)**2), 1)\n",
    "    \n",
    "    data = torch.where(datax < 0, datax*(1-exp(-(z/datax)**2))\n",
    "                         - z*pi**0.5*(erf(z/datax)+1), datax)\n",
    "\n",
    "    if not(torch.all(grad==ddata) and torch.all(grad2==data)): \n",
    "        plt.plot(datax.detach().numpy(), data.detach().numpy()-grad2.detach().numpy())\n",
    "        plt.plot(datax.detach().numpy(), ddata.detach().numpy()-grad.detach().numpy())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040418d-bce5-4219-9a5e-6621bb14d8a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6f9547-20d9-476e-9731-d27b555cbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant block cell\n",
    "if c==1:\n",
    "    plt.plot(datax.detach().numpy(), data.detach().numpy())\n",
    "    plt.plot(datax.detach().numpy(), grad2.detach().numpy())\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(datax.detach().numpy(), ddata.detach().numpy())\n",
    "    plt.plot(datax.detach().numpy(), grad.detach().numpy())\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9673c30f-8d17-470e-ad25-40c9450cc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Softplusplus graph https://www.sciencedirect.com/science/article/pii/S0925231219317163\n",
    "class softplusplus(torch.autograd.Function):\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, data:tensor):\n",
    "\n",
    "        function_val = 1 + exp(data)\n",
    "        \n",
    "        ctx.save_for_backward(function_val)\n",
    "\n",
    "        return log(function_val)+data/2-nplog(2)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:tensor):\n",
    "        (function_val,) = ctx.saved_tensors\n",
    "        \n",
    "        grad = 1.5-1/function_val\n",
    "\n",
    "        return grad*grad_output\n",
    "\n",
    "#Our custom grad function \n",
    "class supeRelu(torch.autograd.Function):\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, data:tensor):\n",
    "        \n",
    "        grad = where(data < 0, 1-exp(-POW(data, -2)), 1)\n",
    "        \n",
    "        ctx.save_for_backward(grad)\n",
    "\n",
    "        return data*grad - where(data < 0, pi**(0.5)*(erf(POW(data, -1))+1), 0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:tensor):\n",
    "        \n",
    "        return ctx.saved_tensors[0]*grad_output\n",
    "\n",
    "#Our custom grad function with trainable parameters (note this is a helper function)\n",
    "class supeRelu2(torch.autograd.Function):\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, data:tensor, a, b, c):\n",
    "\n",
    "        ctx.a = a\n",
    "\n",
    "        ctx.b = b \n",
    "\n",
    "        ctx.c = c \n",
    "\n",
    "        val = b*POW(data, -1) \n",
    "\n",
    "        ctx.component1 = where(data < 0, exp(-POW(val, 2)), 0)\n",
    "\n",
    "        ctx.component2 = where(data < 0, (pi**(1/2))*(erf(val)+1), 0)\n",
    "\n",
    "        ctx.component3 = data*(1-c*ctx.component1) - c*b*ctx.component2\n",
    "        \n",
    "        ctx.save_for_backward(data)\n",
    "\n",
    "        return a*ctx.component3\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:tensor):\n",
    "        \n",
    "        (data, ) = ctx.saved_tensors\n",
    "        \n",
    "        grad_x = grad_a = grad_b = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_x = ctx.a*(1-ctx.c*ctx.component1)*grad_output\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a = ctx.component3*grad_output\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_b = ctx.a*ctx.c*ctx.component2*grad_output\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_c = ctx.a*(data*ctx.component1 - ctx.b*ctx.component2)*grad_output\n",
    "\n",
    "        #print(ctx.a, grad_a)\n",
    "        #print(b, grad_b)\n",
    "        \n",
    "        return grad_x, grad_a, grad_b, grad_c\n",
    "        \n",
    "#Our custom grad function with trainable parameters\n",
    "class supeRelu3(nn.Module):\n",
    "\n",
    "    def __init__(self, a = 0, b = 0, c = 0) -> None:\n",
    "        super(supeRelu3, self).__init__()\n",
    "        self.a = nn.Parameter(a*torch.ones(1),requires_grad = True)\n",
    "        self.b = nn.Parameter(b*torch.ones(1),requires_grad = True)\n",
    "        self.c = nn.Parameter(c*torch.ones(1),requires_grad = True)\n",
    "        #self.a = nn.Parameter(torch.rand(1), requires_grad = True)\n",
    "        #self.b = nn.Parameter(b*torch.rand(1), requires_grad = True)\n",
    "        self.fn = supeRelu2.apply\n",
    "\n",
    "    def forward(self, x) -> tensor:\n",
    "\n",
    "        return self.fn(x, exp(self.a), exp(self.b) , exp(self.c))\n",
    "\n",
    "#Our custom grad function helper with \"nice\" parameters \n",
    "class supeRelu4(torch.autograd.Function):\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, data:tensor):\n",
    "\n",
    "        b = (pi)**(-0.5)# 2*(1.5/e)**(1.5) \n",
    "        \n",
    "        grad = where(data < 0, 1-exp(-b**2*POW(data, -2)), 1)\n",
    "        \n",
    "        ctx.save_for_backward(grad)\n",
    "\n",
    "        return data*grad - where(data < 0, b*pi**(0.5)*(erf(b*POW(data, -1))+1), 0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:tensor):\n",
    "        \n",
    "        return ctx.saved_tensors[0]*grad_output\n",
    "\n",
    "#Our custom grad function as a nn.Module \n",
    "class supeRelu5(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(supeRelu5, self).__init__()\n",
    "        self.fn = supeRelu4.apply\n",
    "\n",
    "    def forward(self, x) -> tensor:\n",
    "\n",
    "        return self.fn(x)\n",
    "\n",
    "#Another activation function (unnecessary)\n",
    "class supeLeakRelu(torch.autograd.Function):\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, data:tensor):\n",
    "\n",
    "        ctx.a = 1.5**(0.5) \n",
    "\n",
    "        ctx.b = 0\n",
    "\n",
    "        ctx.c = 0.5\n",
    "\n",
    "        ctx.a1 = ctx.a*ctx.c*e**(ctx.a**2)*pi**(0.5)\n",
    "\n",
    "        data = data + ctx.c \n",
    "        \n",
    "        ctx.mask1 = data <= -ctx.c\n",
    "\n",
    "        ctx.mask2 = ((-ctx.c < data ) & (data  < 0))\n",
    "\n",
    "        ctx.mask3 = ((data < ctx.c) & (data  >= 0))\n",
    "\n",
    "        ctx.mask4 = data >= ctx.c\n",
    "        \n",
    "        grad = ones_like(data) \n",
    "\n",
    "        grad[ctx.mask1] = ctx.a1 \n",
    "\n",
    "        grad[ctx.mask2] = supeLeakRelu.helper_function2(ctx.a, ctx.c, ctx.a1, data[ctx.mask2] + ctx.c) \n",
    "\n",
    "        grad[ctx.mask3] = 2*data[ctx.mask3]-supeLeakRelu.helper_function2(ctx.a, ctx.c, ctx.a1, data[ctx.mask3] - ctx.c) \n",
    "\n",
    "        grad[ctx.mask4] = 2*data[ctx.mask4] + ctx.a1\n",
    "        \n",
    "        ctx.save_for_backward(data)\n",
    "\n",
    "        a2 = ctx.c + ctx.a1*sp_erf(ctx.a)\n",
    "\n",
    "        return ctx.b*data+(1-ctx.b)*0.5*(grad-a2) #- (ctx.b + (ctx.b-ctx.a)*(2+ctx.a1-a2)*0.5)\n",
    "\n",
    "    @staticmethod\n",
    "    def helper_function2(a, c, a1, data:tensor): \n",
    "\n",
    "        return data*supeLeakRelu.helper_function1(a, c, data) + a1*erf(a*c*POW(data, -1))\n",
    "\n",
    "    @staticmethod\n",
    "    def helper_function1(a, c, data:tensor): \n",
    "\n",
    "        return exp(a**2*(1 - c**2*POW(data, -2)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:tensor):\n",
    "\n",
    "        (data, ) = ctx.saved_tensors\n",
    "\n",
    "        dgrad = ones_like(data) \n",
    "        \n",
    "        dgrad[ctx.mask1] = 0\n",
    "\n",
    "        dgrad[ctx.mask2] = supeLeakRelu.helper_function1(ctx.a, ctx.c, data[ctx.mask2] + ctx.c)*0.5\n",
    "\n",
    "        dgrad[ctx.mask3] = 1 - supeLeakRelu.helper_function1(ctx.a, ctx.c, data[ctx.mask3] - ctx.c)*0.5 \n",
    "\n",
    "        dgrad = ctx.b + (1-ctx.b)*dgrad\n",
    "        \n",
    "        return dgrad*grad_output\n",
    "\n",
    "class supeLeakRelu1(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(supeLeakRelu1, self).__init__()\n",
    "        self.fn = supeLeakRelu.apply\n",
    "\n",
    "    def forward(self, x) -> tensor:\n",
    "\n",
    "        return self.fn(x)\n",
    "\n",
    "#squareplus function https://arxiv.org/abs/2112.11687\n",
    "class squarePlus(torch.autograd.Function):\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, data:tensor):\n",
    "\n",
    "        ctx.b = 0.25\n",
    "\n",
    "        val = POW(data, 2) + ctx.b\n",
    "        \n",
    "        ctx.save_for_backward(data, val)\n",
    "\n",
    "        return 0.5*(data + POW(val, 0.5)) #- ctx.b\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:tensor):\n",
    "\n",
    "        (data, val) = ctx.saved_tensors\n",
    "\n",
    "        grad = 0.5*(1 + data*POW(val, -0.5))\n",
    "        \n",
    "        return grad*grad_output\n",
    "\n",
    "class squarePlus1(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(squarePlus1, self).__init__()\n",
    "        self.fn =  squarePlus.apply\n",
    "\n",
    "    def forward(self, x) -> tensor:\n",
    "\n",
    "        return self.fn(x)\n",
    "\n",
    "# https://link.springer.com/article/10.1007/s00521-017-3210-6\n",
    "#Supposedly the best activation function \n",
    "class modifiedElliot(torch.autograd.Function):\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, data:tensor):\n",
    "\n",
    "        a = 1\n",
    "\n",
    "        grad = POW((a + POW(data, 2)), 0.5)\n",
    "\n",
    "        ctx.save_for_backward(grad, data)\n",
    "\n",
    "        return 0.5*(grad + data - a**0.5)\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output:tensor):\n",
    "\n",
    "        (grad, data)  = ctx.saved_tensors\n",
    "\n",
    "        grad = 0.5*(data*POW(grad, -1) + 1) \n",
    "        \n",
    "        return grad*grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9440987-eb26-46ac-a6a0-645c8e975774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3923, -0.2236, -0.3195, -1.2050,  1.0445, -0.6332,  0.5731,  0.5409,\n",
       "        -0.3919, -1.0427,  1.3186,  0.7476, -1.3265, -1.2413, -0.1028],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test function and make sure it works \n",
    "torch.manual_seed(2)\n",
    "\n",
    "#supeReluTest = softplusplus.apply \n",
    "#supeReluTest = supeRelu5()\n",
    "#supeReluTest = supeLeakRelu1()\n",
    "#supeReluTest = softcross.apply \n",
    "supeReluTest = squarePlus1()\n",
    "#supeReluTest = modifiedElliot.apply\n",
    "\n",
    "data = torch.randn(15, dtype=torch.float64, requires_grad=True) \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91883187-30a7-4a84-b607-b962e6532be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    if torch.autograd.gradcheck(supeReluTest, data, eps=1e-8, atol=1e-7): \n",
    "        print(1)\n",
    "    else: \n",
    "        print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27459d47-e527-493d-8993-fe4382479e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda1e9e-2155-4602-9407-c9b5ee3d9a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f8ba85-0cde-445c-8363-7d7560e7cd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7205e19-8b0b-41e3-af4d-e6a77c5b7fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a7bce-fc8a-4812-ba59-4ee20a7ebda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
